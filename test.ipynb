{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64778b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "237ad49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cpu\n",
      "üìä Model: convnextv2_base.fcmae_ft_in22k_in1k_384\n",
      "üñºÔ∏è  Image Size: 384x384\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    IMAGE_DIR = 'path/to/durian_images/'  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô path ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "    CSV_FILE = 'durian_leaf.csv'\n",
    "    SAVE_DIR = 'models/'\n",
    "    \n",
    "    # Model - ConvNeXt V2 Options\n",
    "    MODEL_NAME = 'convnextv2_base.fcmae_ft_in22k_in1k_384'  # ‚≠ê ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥\n",
    "    # MODEL_NAME = 'convnextv2_tiny.fcmae_ft_in22k_in1k'\n",
    "    # MODEL_NAME = 'convnextv2_large.fcmae_ft_in22k_in1k_384'\n",
    "    # MODEL_NAME = 'convnextv2_huge.fcmae_ft_in22k_in1k_384'  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU ‡πÅ‡∏£‡∏á\n",
    "    \n",
    "    NUM_CLASSES = 4\n",
    "    IMG_SIZE = 384  # ConvNeXt V2 ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö 384x384\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    BATCH_SIZE = 16  # ‡∏•‡∏î‡∏•‡∏á‡∏ñ‡πâ‡∏≤ GPU ‡πÑ‡∏°‡πà‡∏û‡∏≠\n",
    "    EPOCHS = 100\n",
    "    LEARNING_RATE = 1e-4\n",
    "    MIN_LR = 1e-6\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    WARMUP_EPOCHS = 5\n",
    "    \n",
    "    # Advanced Training\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "    MIXUP_ALPHA = 0.2\n",
    "    CUTMIX_ALPHA = 1.0\n",
    "    MIXUP_PROB = 0.5\n",
    "    \n",
    "    # Cross Validation\n",
    "    N_FOLDS = 5\n",
    "    SEED = 42\n",
    "    \n",
    "    # System\n",
    "    NUM_WORKERS = 4\n",
    "    PIN_MEMORY = True\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Early Stopping\n",
    "    PATIENCE = 15\n",
    "    \n",
    "    # TTA (Test Time Augmentation)\n",
    "    USE_TTA = True\n",
    "    TTA_STEPS = 5\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(config.SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üöÄ Using device: {config.DEVICE}\")\n",
    "print(f\"üìä Model: {config.MODEL_NAME}\")\n",
    "print(f\"üñºÔ∏è  Image Size: {config.IMG_SIZE}x{config.IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cd41eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Dataset ====================\n",
    "class DurianLeafDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, mixup_fn=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.mixup_fn = mixup_fn\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_name = row['id']\n",
    "        if not img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_name = img_name + '.jpg'  # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "        \n",
    "        img_path = os.path.join(config.IMAGE_DIR, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Get label\n",
    "        label = int(row['predict'])\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24524055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Data Augmentation ====================\n",
    "def get_train_transforms():\n",
    "    \"\"\"Training transforms with strong augmentation\"\"\"\n",
    "    return A.Compose([\n",
    "        # Resize and basic transforms\n",
    "        A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        \n",
    "        # Geometric transforms\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1,\n",
    "            scale_limit=0.2,\n",
    "            rotate_limit=45,\n",
    "            border_mode=0,\n",
    "            p=0.7\n",
    "        ),\n",
    "        \n",
    "        # Advanced augmentations\n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(alpha=1, sigma=50, p=1),\n",
    "            A.GridDistortion(p=1),\n",
    "            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n",
    "        ], p=0.3),\n",
    "        \n",
    "        # Color augmentations\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.3,\n",
    "                contrast_limit=0.3,\n",
    "                p=1\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=20,\n",
    "                sat_shift_limit=30,\n",
    "                val_shift_limit=20,\n",
    "                p=1\n",
    "            ),\n",
    "            A.RGBShift(\n",
    "                r_shift_limit=20,\n",
    "                g_shift_limit=20,\n",
    "                b_shift_limit=20,\n",
    "                p=1\n",
    "            ),\n",
    "        ], p=0.5),\n",
    "        \n",
    "        # Noise and blur\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=1),\n",
    "            A.MotionBlur(blur_limit=7, p=1),\n",
    "            A.MedianBlur(blur_limit=7, p=1),\n",
    "        ], p=0.3),\n",
    "        \n",
    "        # Weather effects (simulating outdoor conditions)\n",
    "        A.OneOf([\n",
    "            A.RandomRain(p=1),\n",
    "            A.RandomShadow(p=1),\n",
    "            A.RandomFog(p=1),\n",
    "        ], p=0.2),\n",
    "        \n",
    "        # Cutout\n",
    "        A.CoarseDropout(\n",
    "            max_holes=8,\n",
    "            max_height=32,\n",
    "            max_width=32,\n",
    "            fill_value=0,\n",
    "            p=0.3\n",
    "        ),\n",
    "        \n",
    "        # Normalize (ImageNet stats)\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_valid_transforms():\n",
    "    \"\"\"Validation transforms - minimal augmentation\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_tta_transforms():\n",
    "    \"\"\"Test-Time Augmentation transforms\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84ecf6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Mixup & CutMix ====================\n",
    "class MixupCutmix:\n",
    "    def __init__(self, mixup_alpha=0.2, cutmix_alpha=1.0, prob=0.5, num_classes=4):\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_alpha = cutmix_alpha\n",
    "        self.prob = prob\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        if np.random.rand() > self.prob:\n",
    "            return batch\n",
    "        \n",
    "        images, labels = batch\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            # Mixup\n",
    "            return self.mixup(images, labels)\n",
    "        else:\n",
    "            # CutMix\n",
    "            return self.cutmix(images, labels)\n",
    "    \n",
    "    def mixup(self, images, labels):\n",
    "        batch_size = images.size(0)\n",
    "        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "        \n",
    "        index = torch.randperm(batch_size).to(images.device)\n",
    "        mixed_images = lam * images + (1 - lam) * images[index]\n",
    "        \n",
    "        labels_a = torch.zeros(batch_size, self.num_classes).to(images.device)\n",
    "        labels_b = torch.zeros(batch_size, self.num_classes).to(images.device)\n",
    "        labels_a.scatter_(1, labels.unsqueeze(1), 1)\n",
    "        labels_b.scatter_(1, labels[index].unsqueeze(1), 1)\n",
    "        \n",
    "        mixed_labels = lam * labels_a + (1 - lam) * labels_b\n",
    "        \n",
    "        return mixed_images, mixed_labels\n",
    "    \n",
    "    def cutmix(self, images, labels):\n",
    "        batch_size = images.size(0)\n",
    "        lam = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n",
    "        \n",
    "        index = torch.randperm(batch_size).to(images.device)\n",
    "        \n",
    "        # Get random box\n",
    "        _, _, H, W = images.shape\n",
    "        cut_rat = np.sqrt(1.0 - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "        \n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        \n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        \n",
    "        # Apply cutmix\n",
    "        mixed_images = images.clone()\n",
    "        mixed_images[:, :, bby1:bby2, bbx1:bbx2] = images[index, :, bby1:bby2, bbx1:bbx2]\n",
    "        \n",
    "        # Adjust lambda\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "        \n",
    "        labels_a = torch.zeros(batch_size, self.num_classes).to(images.device)\n",
    "        labels_b = torch.zeros(batch_size, self.num_classes).to(images.device)\n",
    "        labels_a.scatter_(1, labels.unsqueeze(1), 1)\n",
    "        labels_b.scatter_(1, labels[index].unsqueeze(1), 1)\n",
    "        \n",
    "        mixed_labels = lam * labels_a + (1 - lam) * labels_b\n",
    "        \n",
    "        return mixed_images, mixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "444a411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Model ====================\n",
    "class ConvNeXtV2Classifier(nn.Module):\n",
    "    def __init__(self, model_name=config.MODEL_NAME, num_classes=config.NUM_CLASSES, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load ConvNeXt V2 from timm\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            drop_path_rate=0.2  # Stochastic depth for regularization\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Loaded model: {model_name}\")\n",
    "        print(f\"üìä Number of parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37eaac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Training Functions ====================\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, device, mixup_fn, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f'Training Epoch {epoch}')\n",
    "    \n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Apply Mixup/CutMix\n",
    "        use_mixup = np.random.rand() < config.MIXUP_PROB\n",
    "        if use_mixup:\n",
    "            images, labels_mixed = mixup_fn((images, labels))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # For mixed labels, use KL divergence or cross entropy with soft labels\n",
    "            loss = -(labels_mixed * torch.log_softmax(outputs, dim=1)).sum(dim=1).mean()\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if not use_mixup:\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    train_acc = 100 * correct / total if total > 0 else 0\n",
    "    return running_loss / len(loader), train_acc\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validation'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return running_loss / len(loader), accuracy * 100, f1, all_preds, all_labels\n",
    "\n",
    "def validate_with_tta(model, loader, device, num_tta=5):\n",
    "    \"\"\"Validation with Test-Time Augmentation\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='TTA Validation'):\n",
    "            batch_preds = []\n",
    "            \n",
    "            # Original prediction\n",
    "            images_device = images.to(device)\n",
    "            outputs = model(images_device)\n",
    "            batch_preds.append(torch.softmax(outputs, dim=1))\n",
    "            \n",
    "            # TTA augmentations\n",
    "            for _ in range(num_tta - 1):\n",
    "                # Horizontal flip\n",
    "                flipped = torch.flip(images_device, dims=[3])\n",
    "                outputs = model(flipped)\n",
    "                batch_preds.append(torch.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Average predictions\n",
    "            avg_pred = torch.stack(batch_preds).mean(dim=0)\n",
    "            _, predicted = torch.max(avg_pred, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return accuracy * 100, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23dfc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Main Training ====================\n",
    "def train_model():\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(config.CSV_FILE)\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    class_dist = df['predict'].value_counts().sort_index()\n",
    "    class_names = ['‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏£‡∏Ñ', '‡πÅ‡∏°‡∏•‡∏á', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤', '‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢']\n",
    "    for idx, count in class_dist.items():\n",
    "        print(f\"  Class {idx} ({class_names[idx]}): {count} samples ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.SEED)\n",
    "    \n",
    "    fold_results = []\n",
    "    all_fold_preds = []\n",
    "    all_fold_labels = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['predict'])):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üî• Fold {fold + 1}/{config.N_FOLDS}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        train_df = df.iloc[train_idx]\n",
    "        val_df = df.iloc[val_idx]\n",
    "        \n",
    "        # Datasets\n",
    "        train_dataset = DurianLeafDataset(train_df, transform=get_train_transforms())\n",
    "        val_dataset = DurianLeafDataset(val_df, transform=get_valid_transforms())\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=config.NUM_WORKERS,\n",
    "            pin_memory=config.PIN_MEMORY\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=config.NUM_WORKERS,\n",
    "            pin_memory=config.PIN_MEMORY\n",
    "        )\n",
    "        \n",
    "        # Model\n",
    "        model = ConvNeXtV2Classifier().to(config.DEVICE)\n",
    "        \n",
    "        # Loss function with class weights\n",
    "        class_counts = train_df['predict'].value_counts().sort_index()\n",
    "        class_weights = torch.FloatTensor([\n",
    "            len(train_df) / (len(class_counts) * count) \n",
    "            for count in class_counts\n",
    "        ]).to(config.DEVICE)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(\n",
    "            weight=class_weights,\n",
    "            label_smoothing=config.LABEL_SMOOTHING\n",
    "        )\n",
    "        \n",
    "        # Mixup/CutMix\n",
    "        mixup_fn = MixupCutmix(\n",
    "            mixup_alpha=config.MIXUP_ALPHA,\n",
    "            cutmix_alpha=config.CUTMIX_ALPHA,\n",
    "            prob=config.MIXUP_PROB,\n",
    "            num_classes=config.NUM_CLASSES\n",
    "        )\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # Scheduler with warmup\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config.EPOCHS - config.WARMUP_EPOCHS,\n",
    "            eta_min=config.MIN_LR\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_acc = 0\n",
    "        best_f1 = 0\n",
    "        patience_counter = 0\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "        \n",
    "        for epoch in range(1, config.EPOCHS + 1):\n",
    "            # Warmup learning rate\n",
    "            if epoch <= config.WARMUP_EPOCHS:\n",
    "                lr = config.LEARNING_RATE * epoch / config.WARMUP_EPOCHS\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "            \n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, criterion, optimizer, \n",
    "                scheduler if epoch > config.WARMUP_EPOCHS else None,\n",
    "                config.DEVICE, mixup_fn, epoch\n",
    "            )\n",
    "            \n",
    "            val_loss, val_acc, val_f1, val_preds, val_labels = validate_epoch(\n",
    "                model, val_loader, criterion, config.DEVICE\n",
    "            )\n",
    "            \n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['val_f1'].append(val_f1)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch}/{config.EPOCHS}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Val F1: {val_f1:.4f}\")\n",
    "            print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_f1': val_f1\n",
    "                }, f'{config.SAVE_DIR}/best_model_fold{fold+1}.pth')\n",
    "                patience_counter = 0\n",
    "                print(f\"‚úÖ Saved best model! (Acc: {val_acc:.2f}%, F1: {val_f1:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= config.PATIENCE:\n",
    "                print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        checkpoint = torch.load(f'{config.SAVE_DIR}/best_model_fold{fold+1}.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Final evaluation with TTA\n",
    "        if config.USE_TTA:\n",
    "            print(f\"\\nüîÑ Running Test-Time Augmentation...\")\n",
    "            tta_acc, tta_f1, tta_preds, tta_labels = validate_with_tta(\n",
    "                model, val_loader, config.DEVICE, config.TTA_STEPS\n",
    "            )\n",
    "            print(f\"TTA Accuracy: {tta_acc:.2f}% | TTA F1: {tta_f1:.4f}\")\n",
    "            \n",
    "            fold_results.append({\n",
    "                'fold': fold + 1,\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'best_f1': best_f1,\n",
    "                'tta_acc': tta_acc,\n",
    "                'tta_f1': tta_f1\n",
    "            })\n",
    "            \n",
    "            all_fold_preds.extend(tta_preds)\n",
    "            all_fold_labels.extend(tta_labels)\n",
    "            \n",
    "            val_preds = tta_preds\n",
    "            val_labels = tta_labels\n",
    "        else:\n",
    "            fold_results.append({\n",
    "                'fold': fold + 1,\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'best_f1': best_f1\n",
    "            })\n",
    "            \n",
    "            all_fold_preds.extend(val_preds)\n",
    "            all_fold_labels.extend(val_labels)\n",
    "        \n",
    "        # Classification Report\n",
    "        print(f\"\\nüìä Classification Report (Fold {fold+1}):\")\n",
    "        print(classification_report(\n",
    "            val_labels, val_preds,\n",
    "            target_names=class_names,\n",
    "            digits=4\n",
    "        ))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(val_labels, val_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_names,\n",
    "                   yticklabels=class_names)\n",
    "        plt.title(f'Confusion Matrix - Fold {fold+1}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{config.SAVE_DIR}/confusion_matrix_fold{fold+1}.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(history['train_acc'], label='Train Acc')\n",
    "        plt.plot(history['val_acc'], label='Val Acc')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "        plt.title('Accuracy')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(history['val_f1'], label='Val F1', color='green')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.legend()\n",
    "        plt.title('F1 Score')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{config.SAVE_DIR}/training_history_fold{fold+1}.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # ==================== Final Results ====================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèÜ FINAL CROSS-VALIDATION RESULTS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    results_df = pd.DataFrame(fold_results)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nüìä Summary Statistics:\")\n",
    "    if config.USE_TTA:\n",
    "        print(f\"Mean Validation Accuracy: {results_df['best_val_acc'].mean():.2f}% ¬± {results_df['best_val_acc'].std():.2f}%\")\n",
    "        print(f\"Mean Validation F1: {results_df['best_f1'].mean():.4f} ¬± {results_df['best_f1'].std():.4f}\")\n",
    "        print(f\"\\nMean TTA Accuracy: {results_df['tta_acc'].mean():.2f}% ¬± {results_df['tta_acc'].std():.2f}%\")\n",
    "        print(f\"Mean TTA F1: {results_df['tta_f1'].mean():.4f} ¬± {results_df['tta_f1'].std():.4f}\")\n",
    "    else:\n",
    "        print(f\"Mean Validation Accuracy: {results_df['best_val_acc'].mean():.2f}% ¬± {results_df['best_val_acc'].std():.2f}%\")\n",
    "        print(f\"Mean Validation F1: {results_df['best_f1'].mean():.4f} ¬± {results_df['best_f1'].std():.4f}\")\n",
    "    \n",
    "    # Overall Classification Report\n",
    "    print(f\"\\nüìä Overall Classification Report (All Folds):\")\n",
    "    print(classification_report(\n",
    "        all_fold_labels, all_fold_preds,\n",
    "        target_names=class_names,\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    # Overall Confusion Matrix\n",
    "    cm_overall = confusion_matrix(all_fold_labels, all_fold_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_overall, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Overall Confusion Matrix (All Folds)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{config.SAVE_DIR}/confusion_matrix_overall.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(f'{config.SAVE_DIR}/cv_results.csv', index=False)\n",
    "    print(f\"\\n‚úÖ Results saved to {config.SAVE_DIR}/cv_results.csv\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a10394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Inference Function ====================\n",
    "def predict_image(model_path, image_path, device=config.DEVICE):\n",
    "    \"\"\"Predict single image with TTA\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    model = ConvNeXtV2Classifier().to(device)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = np.array(image)\n",
    "    \n",
    "    transform = get_valid_transforms()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Original\n",
    "        augmented = transform(image=image)\n",
    "        img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "        output = model(img_tensor)\n",
    "        predictions.append(torch.softmax(output, dim=1))\n",
    "        \n",
    "        # TTA\n",
    "        if config.USE_TTA:\n",
    "            tta_transform = get_tta_transforms()\n",
    "            for _ in range(config.TTA_STEPS - 1):\n",
    "                augmented = tta_transform(image=image)\n",
    "                img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "                output = model(img_tensor)\n",
    "                predictions.appenRetryTContinuepython                \n",
    "                predictions.append(torch.softmax(output, dim=1))\n",
    "    \n",
    "    # Average predictions\n",
    "    avg_pred = torch.stack(predictions).mean(dim=0)\n",
    "    probabilities = avg_pred.cpu().numpy()[0]\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    \n",
    "    class_names = ['‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏£‡∏Ñ', '‡πÅ‡∏°‡∏•‡∏á', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤', '‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢']\n",
    "    \n",
    "    print(f\"\\nüîç Prediction Results:\")\n",
    "    print(f\"Predicted Class: {predicted_class} ({class_names[predicted_class]})\")\n",
    "    print(f\"\\nClass Probabilities:\")\n",
    "    for i, (name, prob) in enumerate(zip(class_names, probabilities)):\n",
    "        print(f\"  {i}. {name}: {prob*100:.2f}%\")\n",
    "    \n",
    "    return predicted_class, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "032eb466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Ensemble Prediction ====================\n",
    "def ensemble_predict(model_paths, image_path, device=config.DEVICE):\n",
    "    \"\"\"Ensemble prediction from multiple folds\"\"\"\n",
    "    \n",
    "    class_names = ['‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏£‡∏Ñ', '‡πÅ‡∏°‡∏•‡∏á', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤', '‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢']\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = np.array(image)\n",
    "    transform = get_valid_transforms()\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        model = ConvNeXtV2Classifier().to(device)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # TTA for each model\n",
    "            fold_predictions = []\n",
    "            \n",
    "            for _ in range(config.TTA_STEPS):\n",
    "                augmented = transform(image=image)\n",
    "                img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "                output = model(img_tensor)\n",
    "                fold_predictions.append(torch.softmax(output, dim=1))\n",
    "            \n",
    "            # Average TTA predictions for this fold\n",
    "            avg_fold_pred = torch.stack(fold_predictions).mean(dim=0)\n",
    "            all_predictions.append(avg_fold_pred)\n",
    "    \n",
    "    # Average predictions across all folds\n",
    "    final_pred = torch.stack(all_predictions).mean(dim=0)\n",
    "    probabilities = final_pred.cpu().numpy()[0]\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    \n",
    "    print(f\"\\nüéØ Ensemble Prediction Results:\")\n",
    "    print(f\"Predicted Class: {predicted_class} ({class_names[predicted_class]})\")\n",
    "    print(f\"Confidence: {probabilities[predicted_class]*100:.2f}%\")\n",
    "    print(f\"\\nAll Class Probabilities:\")\n",
    "    for i, (name, prob) in enumerate(zip(class_names, probabilities)):\n",
    "        bar = '‚ñà' * int(prob * 50)\n",
    "        print(f\"  {i}. {name:12s}: {bar} {prob*100:.2f}%\")\n",
    "    \n",
    "    return predicted_class, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f0f7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Batch Inference ====================\n",
    "def predict_batch(model_path, image_dir, output_csv='predictions.csv', device=config.DEVICE):\n",
    "    \"\"\"Predict on a batch of images\"\"\"\n",
    "    \n",
    "    model = ConvNeXtV2Classifier().to(device)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    class_names = ['‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏£‡∏Ñ', '‡πÅ‡∏°‡∏•‡∏á', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤', '‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢']\n",
    "    \n",
    "    results = []\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    transform = get_valid_transforms()\n",
    "    \n",
    "    for img_file in tqdm(image_files, desc='Predicting'):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # TTA\n",
    "            for _ in range(config.TTA_STEPS):\n",
    "                augmented = transform(image=image)\n",
    "                img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "                output = model(img_tensor)\n",
    "                predictions.append(torch.softmax(output, dim=1))\n",
    "        \n",
    "        # Average predictions\n",
    "        avg_pred = torch.stack(predictions).mean(dim=0)\n",
    "        probabilities = avg_pred.cpu().numpy()[0]\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        confidence = probabilities[predicted_class]\n",
    "        \n",
    "        results.append({\n",
    "            'filename': img_file,\n",
    "            'predicted_class': predicted_class,\n",
    "            'predicted_label': class_names[predicted_class],\n",
    "            'confidence': confidence,\n",
    "            'prob_class_0': probabilities[0],\n",
    "            'prob_class_1': probabilities[1],\n",
    "            'prob_class_2': probabilities[2],\n",
    "            'prob_class_3': probabilities[3]\n",
    "        })\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Predictions saved to {output_csv}\")\n",
    "    print(f\"\\nüìä Prediction Summary:\")\n",
    "    print(results_df['predicted_label'].value_counts())\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b421d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Visualization Functions ====================\n",
    "def visualize_predictions(image_path, model_path, save_path=None):\n",
    "    \"\"\"Visualize prediction with class probabilities\"\"\"\n",
    "    \n",
    "    class_names = ['‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏£‡∏Ñ', '‡πÅ‡∏°‡∏•‡∏á', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤', '‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢']\n",
    "    colors = ['green', 'orange', 'red', 'purple']\n",
    "    \n",
    "    predicted_class, probabilities = predict_image(model_path, image_path)\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Show image\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(f'Predicted: {class_names[predicted_class]}\\nConfidence: {probabilities[predicted_class]*100:.1f}%', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Show probabilities\n",
    "    bars = ax2.barh(class_names, probabilities, color=colors)\n",
    "    ax2.set_xlabel('Probability', fontsize=12)\n",
    "    ax2.set_xlim([0, 1])\n",
    "    ax2.set_title('Class Probabilities', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (bar, prob) in enumerate(zip(bars, probabilities)):\n",
    "        ax2.text(prob + 0.02, i, f'{prob*100:.1f}%', \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Highlight predicted class\n",
    "    bars[predicted_class].set_edgecolor('black')\n",
    "    bars[predicted_class].set_linewidth(3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_class_distribution(df):\n",
    "    \"\"\"Plot class distribution\"\"\"\n",
    "    class_names = ['‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏£‡∏Ñ', '‡πÅ‡∏°‡∏•‡∏á', '‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤', '‡πÄ‡∏û‡∏•‡∏µ‡πâ‡∏¢']\n",
    "    colors = ['green', 'orange', 'red', 'purple']\n",
    "    \n",
    "    class_counts = df['predict'].value_counts().sort_index()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar plot\n",
    "    bars = ax1.bar(range(len(class_counts)), class_counts.values, color=colors)\n",
    "    ax1.set_xticks(range(len(class_counts)))\n",
    "    ax1.set_xticklabels([f'{i}\\n{name}' for i, name in enumerate(class_names)])\n",
    "    ax1.set_ylabel('Count', fontsize=12)\n",
    "    ax1.set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar, count in zip(bars, class_counts.values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(count)}\\n({count/len(df)*100:.1f}%)',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2.pie(class_counts.values, labels=class_names, colors=colors,\n",
    "           autopct='%1.1f%%', startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "    ax2.set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{config.SAVE_DIR}/class_distribution.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "068982de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Model Analysis ====================\n",
    "def analyze_model_performance(results_csv='models/cv_results.csv'):\n",
    "    \"\"\"Analyze and visualize model performance across folds\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(results_csv)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Validation Accuracy per Fold\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(df['fold'], df['best_val_acc'], marker='o', linewidth=2, markersize=8)\n",
    "    ax1.axhline(df['best_val_acc'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df[\"best_val_acc\"].mean():.2f}%')\n",
    "    ax1.fill_between(df['fold'], \n",
    "                     df['best_val_acc'].mean() - df['best_val_acc'].std(),\n",
    "                     df['best_val_acc'].mean() + df['best_val_acc'].std(),\n",
    "                     alpha=0.2, color='red')\n",
    "    ax1.set_xlabel('Fold', fontsize=12)\n",
    "    ax1.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "    ax1.set_title('Validation Accuracy per Fold', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: F1 Score per Fold\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(df['fold'], df['best_f1'], marker='s', color='green', linewidth=2, markersize=8)\n",
    "    ax2.axhline(df['best_f1'].mean(), color='darkgreen', linestyle='--',\n",
    "                label=f'Mean: {df[\"best_f1\"].mean():.4f}')\n",
    "    ax2.fill_between(df['fold'],\n",
    "                     df['best_f1'].mean() - df['best_f1'].std(),\n",
    "                     df['best_f1'].mean() + df['best_f1'].std(),\n",
    "                     alpha=0.2, color='green')\n",
    "    ax2.set_xlabel('Fold', fontsize=12)\n",
    "    ax2.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax2.set_title('F1 Score per Fold', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: TTA Comparison (if available)\n",
    "    if 'tta_acc' in df.columns:\n",
    "        ax3 = axes[1, 0]\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        ax3.bar(x - width/2, df['best_val_acc'], width, label='Validation', alpha=0.8)\n",
    "        ax3.bar(x + width/2, df['tta_acc'], width, label='TTA', alpha=0.8)\n",
    "        ax3.set_xlabel('Fold', fontsize=12)\n",
    "        ax3.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "        ax3.set_title('Validation vs TTA Accuracy', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xticks(x)\n",
    "        ax3.set_xticklabels(df['fold'])\n",
    "        ax3.legend()\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].axis('off')\n",
    "    \n",
    "    # Plot 4: Summary Statistics\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "    üìä Performance Summary\n",
    "    {'='*40}\n",
    "    \n",
    "    Validation Accuracy:\n",
    "      Mean: {df['best_val_acc'].mean():.2f}%\n",
    "      Std:  {df['best_val_acc'].std():.2f}%\n",
    "      Min:  {df['best_val_acc'].min():.2f}%\n",
    "      Max:  {df['best_val_acc'].max():.2f}%\n",
    "    \n",
    "    F1 Score:\n",
    "      Mean: {df['best_f1'].mean():.4f}\n",
    "      Std:  {df['best_f1'].std():.4f}\n",
    "      Min:  {df['best_f1'].min():.4f}\n",
    "      Max:  {df['best_f1'].max():.4f}\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'tta_acc' in df.columns:\n",
    "        stats_text += f\"\"\"\n",
    "    TTA Accuracy:\n",
    "      Mean: {df['tta_acc'].mean():.2f}%\n",
    "      Std:  {df['tta_acc'].std():.2f}%\n",
    "      Improvement: +{df['tta_acc'].mean() - df['best_val_acc'].mean():.2f}%\n",
    "        \"\"\"\n",
    "    \n",
    "    ax4.text(0.1, 0.5, stats_text, fontsize=11, family='monospace',\n",
    "            verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{config.SAVE_DIR}/performance_analysis.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "416a110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåø Durian Leaf Classification with ConvNeXt V2\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (3), usually from a call to set_ticks, does not match the number of labels (4).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Plot class distribution\u001b[39;00m\n\u001b[32m     12\u001b[39m     df = pd.read_csv(config.CSV_FILE)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mplot_class_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müöÄ Starting training...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mplot_class_distribution\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     57\u001b[39m bars = ax1.bar(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(class_counts)), class_counts.values, color=colors)\n\u001b[32m     58\u001b[39m ax1.set_xticks(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(class_counts)))\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43max1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_xticklabels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m ax1.set_ylabel(\u001b[33m'\u001b[39m\u001b[33mCount\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m12\u001b[39m)\n\u001b[32m     61\u001b[39m ax1.set_title(\u001b[33m'\u001b[39m\u001b[33mClass Distribution\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m14\u001b[39m, fontweight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/personal/durian_leaf_disease_classification/.venv/lib/python3.13/site-packages/matplotlib/axes/_base.py:74\u001b[39m, in \u001b[36m_axis_method_wrapper.__set_name__.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/personal/durian_leaf_disease_classification/.venv/lib/python3.13/site-packages/matplotlib/axis.py:2106\u001b[39m, in \u001b[36mAxis.set_ticklabels\u001b[39m\u001b[34m(self, labels, minor, fontdict, **kwargs)\u001b[39m\n\u001b[32m   2102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(locator, mticker.FixedLocator):\n\u001b[32m   2103\u001b[39m     \u001b[38;5;66;03m# Passing [] as a list of labels is often used as a way to\u001b[39;00m\n\u001b[32m   2104\u001b[39m     \u001b[38;5;66;03m# remove all tick labels, so only error for > 0 labels\u001b[39;00m\n\u001b[32m   2105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(locator.locs) != \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) != \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2106\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe number of FixedLocator locations\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2108\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(locator.locs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), usually from a call to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2109\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m set_ticks, does not match\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2110\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m the number of labels (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2111\u001b[39m     tickd = {loc: lab \u001b[38;5;28;01mfor\u001b[39;00m loc, lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(locator.locs, labels)}\n\u001b[32m   2112\u001b[39m     func = functools.partial(\u001b[38;5;28mself\u001b[39m._format_with_dict, tickd)\n",
      "\u001b[31mValueError\u001b[39m: The number of FixedLocator locations (3), usually from a call to set_ticks, does not match the number of labels (4)."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================== Main Execution ====================\n",
    "if __name__ == '__main__':\n",
    "    print(\"üåø Durian Leaf Classification with ConvNeXt V2\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if CSV exists\n",
    "    if not os.path.exists(config.CSV_FILE):\n",
    "        print(f\"‚ùå Error: {config.CSV_FILE} not found!\")\n",
    "        print(\"Please update the CSV_FILE path in Config class.\")\n",
    "    else:\n",
    "        # Plot class distribution\n",
    "        df = pd.read_csv(config.CSV_FILE)\n",
    "        plot_class_distribution(df)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\nüöÄ Starting training...\")\n",
    "        results = train_model()\n",
    "        \n",
    "        # Analyze performance\n",
    "        print(\"\\nüìä Analyzing model performance...\")\n",
    "        analyze_model_performance()\n",
    "        \n",
    "        print(\"\\n‚úÖ Training completed!\")\n",
    "        print(f\"üìÅ Models saved in: {config.SAVE_DIR}\")\n",
    "        \n",
    "        # Example: Single image prediction\n",
    "        # predicted_class, probs = predict_image(\n",
    "        #     'models/best_model_fold1.pth',\n",
    "        #     'path/to/test_image.jpg'\n",
    "        # )\n",
    "        \n",
    "        # Example: Ensemble prediction\n",
    "        # model_paths = [f'models/best_model_fold{i}.pth' for i in range(1, 6)]\n",
    "        # predicted_class, probs = ensemble_predict(\n",
    "        #     model_paths,\n",
    "        #     'path/to/test_image.jpg'\n",
    "        # )\n",
    "        \n",
    "        # Example: Batch prediction\n",
    "        # results = predict_batch(\n",
    "        #     'models/best_model_fold1.pth',\n",
    "        #     'path/to/test_images/',\n",
    "        #     'predictions.csv'\n",
    "        # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "durian_leaf_disease_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
